{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7730dfff-027e-4e81-a349-6cc5feddcf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: bert-extractive-summarizer in /opt/conda/lib/python3.11/site-packages (0.10.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.40.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from bert-extractive-summarizer) (1.4.1.post1)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.11/site-packages (from bert-extractive-summarizer) (3.7.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->bert-extractive-summarizer) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->bert-extractive-summarizer) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->bert-extractive-summarizer) (3.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (1.10.15)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (69.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy->bert-extractive-summarizer) (3.4.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.2.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy->bert-extractive-summarizer) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy->bert-extractive-summarizer) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-extractive-summarizer transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc35b7a4-48ad-4771-b87f-d92bfc5bbc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from summarizer import Summarizer\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ccada61-42fb-4396-ab98-7f60b440aab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"C:\\Users\\USER\\Desktop\\Bahaa_GP\\t5_weights\\3\"\n",
    "test_data_path = 'cleaned_final_sum_test_separate_columns.csv'\n",
    "\n",
    "# Load the model and tokenizer for inference\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, local_files_only=True)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "arabert_prep = ArabertPreprocessor(model_name=\"C:/Users/USER/Desktop/Bahaa_GP/t5_weights/3\")\n",
    "extractive_model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30c3f7d-3823-42bc-9352-a965a1586a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic_word(word):\n",
    "    replacements = {'ة': 'ه', 'أ': 'ا', 'إ': 'ا', 'آ': 'ا', 'ى': 'ى', 'ؤ': 'و', 'ئ': 'ي'}\n",
    "    for old, new in replacements.items():\n",
    "        word = word.replace(old, new)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a9136f-631c-400f-a4cf-a1b053ddbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(summary):\n",
    "    sentences = summary.split('، ')\n",
    "    seen_sentences = set()\n",
    "    result_sentences = []\n",
    "    for sentence in sentences:\n",
    "        normalized_sentence = ' '.join(normalize_arabic_word(word) for word in sentence.split())\n",
    "        if normalized_sentence not in seen_sentences:\n",
    "            seen_sentences.add(normalized_sentence)\n",
    "            words = sentence.split()\n",
    "            seen_words = defaultdict(set)\n",
    "            result_words = []\n",
    "            for word in words:\n",
    "                normalized_word = normalize_arabic_word(word)\n",
    "                if normalized_word not in seen_words[normalized_word]:\n",
    "                    seen_words[normalized_word].add(normalized_word)\n",
    "                    result_words.append(word)\n",
    "            result_sentences.append(' '.join(result_words))\n",
    "    return '، '.join(result_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5b7d6b-7b01-4486-b8a7-2ac193644185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_patterns(summary):\n",
    "    summary = re.sub(r'(بو ابه اخبار الالكترونيه|اليوم السابع|أي نمط آخر غير مرغوب)', '', summary)\n",
    "    summary = ' '.join(summary.split())\n",
    "    return summary\n",
    "\n",
    "def clean_summary(summary):\n",
    "    summary = remove_duplicates(summary)\n",
    "    summary = remove_unwanted_patterns(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e04922-f1eb-4cbd-ab24-7f9011142778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, n_keywords=10):\n",
    "    if not text.strip() or len(text.split()) < 3: \n",
    "        return []\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=n_keywords, stop_words=None)\n",
    "    try:\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        dense = tfidf_matrix.todense()\n",
    "        dense_list = dense.tolist()[0]\n",
    "        keywords = [feature_names[i] for i in sorted(range(len(dense_list)), key=lambda i: dense_list[i], reverse=True)[:n_keywords]]\n",
    "    except ValueError as e:\n",
    "        print(f\"Error extracting keywords: {e}\")\n",
    "        return []\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    pos_keywords = [word for word, pos in tagged if pos in ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']]\n",
    "    combined_keywords = list(set(keywords + pos_keywords))\n",
    "    return combined_keywords[:n_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "704ded9e-690d-495a-b6ca-719fc6bdb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_target_length(original_text):\n",
    "    length = len(original_text.split())\n",
    "    target_min_length = max(40, int(length * 0.3))  \n",
    "    target_max_length = max(100, int(length * 0.6))  \n",
    "    return target_min_length, target_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2513f202-8444-467d-a34f-a53ee295d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_summary(row):\n",
    "    text = row['Original']\n",
    "    if pd.isna(row['GeneratedSummary']) or row['GeneratedSummary'] == '':\n",
    "        preprocessed_text = arabert_prep.preprocess(text)\n",
    "        extractive_summary = extractive_model(preprocessed_text, min_length=60, max_length=200)\n",
    "\n",
    "        if not extractive_summary.strip():\n",
    "            extractive_summary = preprocessed_text\n",
    "\n",
    "        keywords = extract_keywords(extractive_summary, n_keywords=10)\n",
    "        keywords_prompt = \" \".join(keywords)\n",
    "        inputs = tokenizer.encode(f\"summarize: {extractive_summary} {keywords_prompt}\", return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        attention_mask = inputs != tokenizer.pad_token_id\n",
    "        target_min_length, target_max_length = calculate_target_length(preprocessed_text)\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=target_max_length,\n",
    "            min_length=target_min_length,\n",
    "            length_penalty=1.5,\n",
    "            num_beams=7,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        summary = clean_summary(summary)\n",
    "        row['GeneratedSummary'] = summary\n",
    "        test_data.loc[test_data['Original'] == text, 'GeneratedSummary'] = summary\n",
    "        test_data.to_csv(test_data_path, index=False)\n",
    "        print(f\"Summary generated and saved for text: {text[:50]}...\")\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce4b82-cfce-4898-a93d-27b351371715",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'GeneratedSummary' not in test_data.columns:\n",
    "    test_data['GeneratedSummary'] = ''\n",
    "test_data.apply(generate_and_save_summary, axis=1)\n",
    "print(\"Summaries generated and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49924c1d-ae1d-45ea-8959-367c1bd71866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
