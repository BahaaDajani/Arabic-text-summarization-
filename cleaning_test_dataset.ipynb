{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e4fb27-fff0-4425-a53b-ff3f713b9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "file_path = 'final_sum_test.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "data['Original'] = data['Original'].apply(clean_text)\n",
    "data['Summary.1'] = data['Summary.1'].apply(clean_text)\n",
    "\n",
    "cleaned_file_path = 'cleaned_final_sum_test_separate_columns.csv'\n",
    "data[['Original', 'Summary.1']].to_csv(cleaned_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25732c1-eea5-4942-8547-a5b55cda77a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.11/site-packages (0.1.2)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk->rouge_score) (2024.4.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m995.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets rouge_score evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b944d79-cdea-4f82-92c3-2cf0ac599ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b863789c-dfe6-418d-a0b9-10b97ba60dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            original  \\\n",
      "0  وتحت عنوان من الكارثة إلى التحدى يبدأ الكاتب ع...   \n",
      "1  ولم يعترف دبلوماسيو هاتين الدولتين بالعريضة ال...   \n",
      "2  قامت ولاية حلب بعد اعلان الجنرال الفرنسي هنري ...   \n",
      "3  دولة مصر العربيه هي ليست اي دوله وليست اي شعب ...   \n",
      "4  السوريون يصرون على استقلال بلادهم : و مثلما رف...   \n",
      "\n",
      "                                             summary  \n",
      "0  يبدأ الكاتب عرض الكتاب الرابع تحت عنوان من الك...  \n",
      "1  دبلوماسيو الدولتين لم يعترفوا بالعريضة التي قا...  \n",
      "2  أعلن غورو الانتداب الفرنسي على سوريا لكي يعاقب...  \n",
      "3  مصر هي أم البلاد، وقائدة العرب؛ فهي أرض بلاد ا...  \n",
      "4  الشعب السوري يصر على استقلال بلدهم من السيطرة ...  \n",
      "Index(['original', 'summary'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset from CSV file\n",
    "data_path = \"final_merged_summary.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset and column names\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760dfd92-1c19-4bcd-a036-2a7794e36c15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            original  \\\n",
      "0  وتحت عنوان من الكارثة إلى التحدى يبدأ الكاتب ع...   \n",
      "1  ولم يعترف دبلوماسيو هاتين الدولتين بالعريضة ال...   \n",
      "2  قامت ولاية حلب بعد اعلان الجنرال الفرنسي هنري ...   \n",
      "3  دولة مصر العربيه هي ليست اي دوله وليست اي شعب ...   \n",
      "4  السوريون يصرون على استقلال بلادهم : و مثلما رف...   \n",
      "\n",
      "                                             summary  \n",
      "0  يبدأ الكاتب عرض الكتاب الرابع تحت عنوان من الك...  \n",
      "1  دبلوماسيو الدولتين لم يعترفوا بالعريضة التي قا...  \n",
      "2  أعلن غورو الانتداب الفرنسي على سوريا لكي يعاقب...  \n",
      "3  مصر هي أم البلاد، وقائدة العرب؛ فهي أرض بلاد ا...  \n",
      "4  الشعب السوري يصر على استقلال بلدهم من السيطرة ...  \n",
      "Index(['original', 'summary'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Model provided is not in the accepted model list. Preprocessor will default to a base Arabic preprocessor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda4b24b65db4b65a0eac1d1aaaad128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/275274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190159725d9c411f9e5a0ba8e47c7c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import Dataset\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"malmarjeh/t5-arabic-text-summarization\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load dataset from CSV file\n",
    "data_path = \"final_merged_summary.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset and column names\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "# Rename columns to match the expected column names in preprocess_function\n",
    "df.columns = ['original', 'summary']\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Preprocess data\n",
    "arabert_prep = ArabertPreprocessor(model_name.split('/')[1])\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Preprocess the input text\n",
    "    inputs = [\"summarize: \" + arabert_prep.preprocess(doc) for doc in examples[\"original\"]]\n",
    "    \n",
    "    # Ensure summaries are in the correct format and convert to strings\n",
    "    summaries = [str(summary) for summary in examples[\"summary\"]]\n",
    "    \n",
    "    # Tokenize inputs and summaries\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(summaries, max_length=150, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Add labels to model_inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_rouge(model, dataset):\n",
    "    model.eval()\n",
    "    predictions, references = [], []\n",
    "    \n",
    "    for example in dataset:\n",
    "        input_ids = torch.tensor(example['input_ids']).unsqueeze(0)\n",
    "        outputs = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        reference = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "    \n",
    "    rouge_output = rouge.compute(predictions=predictions, references=references)\n",
    "    return rouge_output\n",
    "\n",
    "# Evaluate the original model\n",
    "original_model_scores = compute_rouge(model, tokenized_dataset)\n",
    "print(\"Original Model ROUGE Scores:\", original_model_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e91ca-e071-4d4c-b3c8-4a13ccf1ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last checkpoint\n",
    "checkpoint_directory = r\"C:\\Users\\USER\\Desktop\\Bahaa_GP\\t5_weights\\3\"\n",
    "last_checkpoint_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_directory)\n",
    "last_checkpoint_tokenizer = AutoTokenizer.from_pretrained(checkpoint_directory)\n",
    "\n",
    "# Evaluate the last checkpoint model\n",
    "last_checkpoint_scores = compute_rouge(last_checkpoint_model, tokenized_dataset)\n",
    "print(\"Last Checkpoint ROUGE Scores:\", last_checkpoint_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab520901-f7c5-4570-9a33-445482dd9c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
